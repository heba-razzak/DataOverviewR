# install package from github
# library(devtools)
install_github("heba-razzak/lim-lab/createDataDict")
library(dplyr) # For data manipulation
library(sf) # For working with spatial data
library(data.table)
# install package from github
library(devtools)
install_github("heba-razzak/lim-lab/createDataDict")
library(createDataDict)
# install package from local files
# source("createDataDict/createDataDict.R") # print_descriptions_df & print_data_dict functions
setwd('/Users/heba/Desktop/Uni/Lim Lab/OSM')
# read roads file
sanfrancell_roads <- st_read("grid238_roads_osm.shp", quiet = TRUE)
# print skeleton for filling in descriptions
print_descriptions_df(sanfrancell_roads)
descriptions = data.frame(Variable = c('osm_id',
'name',
'highway',
'lanes',
'maxspeed',
'geom'),
Description = c('osm_id_description',
'name_description',
'highway_description',
'lanes_description',
'maxspeed_description',
'geom_description'))
print_data_dict(sanfrancell_roads, title="OSM Roads", descriptions=descriptions)
print_data_dict(sanfrancell_roads, title="OSM Roads", descriptions=NULL)
knitr::opts_knit$set(root.dir = "/Users/heba/Desktop/Uni/Lim Lab/Purple Air")
suppressPackageStartupMessages({
library(mapview) # For interactive maps
library(rjson) # For working with JSON data
library(httr) # For making HTTP requests
library(sf) # For working with spatial data
library(dplyr)
library(tidycensus) # For accessing US Census data
library(tidyverse) # For data manipulation and visualization
library(lubridate) # For working with dates
library(ggplot2) # For visualizing data
library(magick) # images/gifs
})
# load getPurpleairApiHistoryV2 function from folder it's in
source("getPurpleairApiHistory/getPurpleairApiHistoryV2.R")
crs = 4326
# Store the URL of the API endpoint to request data from for PurpleAir air quality sensors
all <- "https://api.purpleair.com/v1/sensors?fields=latitude%2C%20longitude%2C%20date_created%2C%20last_seen"
# Define the API key used to authenticate the user's request to the PurpleAir API
auth_key  <- "2C4E0A86-014A-11ED-8561-42010A800005"
# Define the header for the HTTP request to the API, including the API key and Accept content type
header = c(
'X-API-Key' = auth_key,
'Accept' = "application/json"
)
# Get Purple Air data using the following steps
# Make the HTTP request to the PurpleAir API using the GET function from the httr library
# Convert the raw content returned by the API into a character string
# Convert the character string into a JSON object
# Extract the "data" element from the JSON object and convert it to a data frame
result <- GET(all, add_headers(header))
raw <- rawToChar(result$content)
json <- jsonlite::fromJSON(raw)
pa <- as.data.frame(json$data)
# Rename the columns of the PurpleAir data frame
colnames(pa) <- c("sensor_id","date_created", "last_seen", "lat", "lon")
# convert epoch timestamp to date
pa$date_created <- as.Date(as.POSIXct(pa$date_created, origin = "1970-01-01"))
pa$last_seen <- as.Date(as.POSIXct(pa$last_seen, origin = "1970-01-01"))
# Convert the PurpleAir data frame to an sf object
pa <- pa %>% na.omit()
dt <- st_as_sf(pa, coords=c("lon", "lat"), crs = crs)
print(dt)
head(dt)
# greater san fran area
# coordinates used for OSM
# bbox <- c(left = -123.8, bottom = 36.9, right = -121.0, top = 39.0)
# Greater san fran area
bbox <- c(xmin = -123.8, ymin = 36.9, xmax = -121.0, ymax = 39.0)
# Shapefile of bounding box
bbox_sf <- st_as_sfc(st_bbox(bbox))
# Set CRS (coordinate reference system)
crs = 4326
st_crs(bbox_sf) <- crs
# bbox <- c(left = -124.0146, bottom = 36.5217, right = -120.9834, top = 39.13)
#
# x = list(rbind(c(bbox["left"],bbox["bottom"]),
#                c(bbox["left"],bbox["top"]),
#                c(bbox["right"],bbox["top"]),
#                c(bbox["right"],bbox["bottom"]),
#                c(bbox["left"],bbox["bottom"])))
#
# # Create a polygon for san fran area
# bbox_sf <- st_polygon(x)
#
# # convert to sf object
# bbox_sf <- st_sfc(bbox_sf, crs=crs)
# intersection of purple air sensors and bounding box
purpleairs_sf <- st_intersection(dt, bbox_sf)
# interactive map
mapview(purpleairs_sf)
# Greater san fran area
bbox <- c(xmin = -123.8, ymin = 36.9, xmax = -121.0, ymax = 39.0)
# Shapefile of bounding box
bbox_sf <- st_as_sfc(st_bbox(bbox))
# Set CRS (coordinate reference system)
crs = 4326
st_crs(bbox_sf) <- crs
# intersection of purple air sensors and bounding box
purpleairs_sf <- st_intersection(dt, bbox_sf)
# interactive map
mapview(purpleairs_sf)
knitr::opts_knit$set(root.dir = "/Users/heba/Desktop/Uni/Lim Lab/Purple Air")
auth_key <- "2C4E0A86-014A-11ED-8561-42010A800005"
suppressPackageStartupMessages({
library(mapview) # For interactive maps
library(rjson) # For working with JSON data
library(httr) # For making HTTP requests
library(sf) # For working with spatial data
library(dplyr)
library(tidycensus) # For accessing US Census data
library(tidyverse) # For data manipulation and visualization
library(lubridate) # For working with dates
library(ggplot2) # For visualizing data
library(magick) # images/gifs
})
# load getPurpleairApiHistoryV2 function from folder it's in
source("getPurpleairApiHistory/getPurpleairApiHistoryV2.R")
# Store the URL of the API endpoint to request data from for PurpleAir air quality sensors
all <- "https://api.purpleair.com/v1/sensors?fields=latitude%2C%20longitude%2C%20date_created%2C%20last_seen"
# Define the API key used to authenticate the user's request to the PurpleAir API
# auth_key  <- "XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX"
# Define the header for the HTTP request to the API, including the API key and Accept content type
header = c(
'X-API-Key' = auth_key,
'Accept' = "application/json"
)
# Get Purple Air data using the following steps
# Make the HTTP request to the PurpleAir API using the GET function from the httr library
# Convert the raw content returned by the API into a character string
# Convert the character string into a JSON object
# Extract the "data" element from the JSON object and convert it to a data frame
result <- GET(all, add_headers(header))
raw <- rawToChar(result$content)
json <- jsonlite::fromJSON(raw)
pa <- as.data.frame(json$data)
# Rename the columns of the PurpleAir data frame
colnames(pa) <- c("sensor_id","date_created", "last_seen", "lat", "lon")
# convert epoch timestamp to date
pa$date_created <- as.Date(as.POSIXct(pa$date_created, origin = "1970-01-01"))
pa$last_seen <- as.Date(as.POSIXct(pa$last_seen, origin = "1970-01-01"))
# CRS (coordinate reference system)
crs = 4326
# Convert the PurpleAir data frame to an sf object
pa <- pa %>% na.omit()
dt <- st_as_sf(pa, coords=c("lon", "lat"), crs = crs)
head(dt)
# Greater san fran area
bbox <- c(xmin = -123.8, ymin = 36.9, xmax = -121.0, ymax = 39.0)
# Shapefile of bounding box
bbox_sf <- st_as_sfc(st_bbox(bbox))
# Set CRS (coordinate reference system)
crs = 4326
st_crs(bbox_sf) <- crs
# intersection of purple air sensors and bounding box
purpleairs_sf <- st_intersection(dt, bbox_sf)
# interactive map
mapview(purpleairs_sf)
length(unique(purpleairs_sf$sensor_id))
auth_key
# Inputs for purple air function
apiReadKey <- auth_key
fields <- c("pm2.5_atm, pm2.5_atm_a, pm2.5_atm_b")
average <- "60"
# Date range of historical purple air data
start_date <- as.Date("2019-12-01")
end_date <- as.Date("2019-12-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
# Date range of historical purple air data
start_date <- as.Date("2018-01-01")
end_date <- as.Date("2018-01-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
# Date range of historical purple air data
start_date <- as.Date("2016-01-01")
end_date <- as.Date("2016-01-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
# Date range of historical purple air data
start_date <- as.Date("2016-01-01")
end_date <- as.Date("2016-01-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
# Date range of historical purple air data
start_date <- as.Date("2017-01-01")
end_date <- as.Date("2016-07-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
# Date range of historical purple air data
start_date <- as.Date("2017-01-01")
end_date <- as.Date("2016-07-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
a
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
sensorIndex
# Date range of historical purple air data
start_date <- as.Date("2017-01-01")
end_date <- as.Date("2017-01-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
purple_air
file_directory = "/Users/heba/Desktop/Uni/Lim Lab/Purple Air"
auth_key <- "2C4E0A86-014A-11ED-8561-42010A800005"
file_directory = "/Users/heba/Desktop/Uni/Lim Lab/Purple Air"
knitr::opts_knit$set(root.dir = file_directory)
# Get a list of file paths
file_paths <- list.files(file_directory, pattern = "purple_air_sanfran_.*.csv", full.names = TRUE)
# Read files
dfs <- lapply(file_paths, read.csv)
# Bind to 1 dataframe
fulldata <- do.call(rbind, dfs)
# Add column for month
fulldata$month <- format(as.Date(fulldata$time_stamp), "%Y-%m")
# Sensors for each month
monthly_sensors <- fulldata %>% select(month, sensor_id) %>% distinct()
head(monthly_sensors)
sensor_counts <- monthly_sensors %>%
group_by(month) %>%
summarise(sensor_count = n_distinct(sensor_id))
ggplot(sensor_counts, aes(x = month, y = sensor_count)) +
geom_bar(stat = "identity", fill = "lavender", color = "black") +
labs(title = "Number of Sensors per Month",
x = "Month",
y = "Number of Sensors") +
scale_y_continuous(breaks = seq(0, max(sensor_counts$sensor_count) + 100, by = 100)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
cat("Total number of sensors: ", length(unique(purpleairs_sf$sensor_id)))
# Date range of historical purple air data
start_date <- as.Date("2017-01-01")
end_date <- as.Date("2017-01-31")
current_date <- start_date
# Iterate over each 1 month period
while (current_date <= end_date) {
# Calculate next date
next_date <- current_date + months(1) - days(1)
# Ensure we don't go beyond the end date
if (next_date > end_date) {
next_date <- end_date
}
# Print the dates we're processing
print(paste("Processing:", current_date, "-", next_date))
start_time <- Sys.time()
filtered_purpleairs_sf <- purpleairs_sf %>% filter(last_seen >= current_date) %>% filter(date_created <= next_date)
sensorIndex <- unique(filtered_purpleairs_sf$sensor_id)
# Get the data
purple_air <- getPurpleairApiHistoryV2(
sensorIndex=sensorIndex,
apiReadKey=apiReadKey,
startTimeStamp=format(current_date, "%Y-%m-%d %H:%M:%S"),
endTimeStamp=format(next_date, "%Y-%m-%d %H:%M:%S"),
average=average,
fields=fields
)
# Save to CSV file
write.csv(purple_air, file = paste0("purple_air_sanfran_", current_date, "_", next_date, ".csv"), row.names = FALSE)
# Print time it took
end_time <- Sys.time()
time_difference <- end_time - start_time
print(paste("Processing time:", current_date, "-", next_date))
print(time_difference)
# Update the current date
current_date <- next_date + days(1)
}
?tidyverse
??tidyverse
